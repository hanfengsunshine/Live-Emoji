
<!doctype html>
<html lang="en">
	<head>
		<title>Face tracker</title>
		<meta charset="utf-8">
		<link href="./styles/bootstrap.min.css" rel="stylesheet" type="text/css">
		<style>
			@import url(https://fonts.googleapis.com/css?family=Lato:300italic,700italic,300,700);
			body {
				font-family: 'Lato';
				background-color: #f0f0f0;
				margin: 0px auto;
				max-width: 1150px;
			}
			#overlay {
				position: absolute;
				top: 0px;
				left: 0px;
				-o-transform : scaleX(-1);
				-webkit-transform : scaleX(-1);
				transform : scaleX(-1);
				-ms-filter : fliph; /*IE*/
				filter : fliph; /*IE*/
			}
			#videoel {
				-o-transform : scaleX(-1);
				-webkit-transform : scaleX(-1);
				transform : scaleX(-1);
				-ms-filter : fliph; /*IE*/
				filter : fliph; /*IE*/
			}
			#container {
				position : relative;
				width : 370px;
			}
			#content { /*设置内容在页面的位置，这里偏右上方*/
				margin-top : 10px;
				margin-left : 700px;
				margin-right : 5px;
				max-width: 1200px;
			}
			h2 {
				font-weight : 400;
			}
			.nogum {
				display : none;
			}
			.btn {
				font-family: 'Lato';
				font-size: 16px;
			}
			.hide {
				display : none;
			}
			.nohide {
				display : block;
			}
		</style>
		<script>
			// getUserMedia only works over https in Chrome 47+, so we redirect to https. Also notify user if running from file.
			if (window.location.protocol == "file:") {
				alert("You seem to be running this example directly from a file. Note that these examples only work when served from a server or localhost due to canvas cross-domain restrictions.");
			} else if (window.location.hostname !== "localhost" && window.location.protocol !== "https:"){
				window.location.protocol = "https";
			}
		</script>
		<script type="text/javascript">
			var _gaq = _gaq || [];
			_gaq.push(['_setAccount', 'UA-32642923-1']);
			_gaq.push(['_trackPageview']);
			(function() {
				var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
				ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
				var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
			})();
		</script>
	</head>
	<body><!-- 这里是要include的一些js及其位置 --> 
		<script src="./js/utils.js"></script>
		<script src="./js/headtrackr.js"></script>
		<script src="./js/model_pca_20_svm.js"></script>
		<script src="./js/clmtrackr.js"></script>
		<script src="./js/Stats.js"></script>
		<script src="./js/emotion_classifier.js"></script>
		<script src="./js/emotionmodel.js"></script>
		
		<div id="content">
			<div id="container"><!-- 这里设置video和canvas的container --> 
				<video id="videoel" width="400" height="300" preload="auto" loop playsinline autoplay>
				</video>
				<canvas id="overlay" width="400" height="300"></canvas>
			</div>
			<br/>
			<input class="btn" type="button" value="wait, loading video" disabled="disabled" onclick="startVideo()" id="startbutton"></input> <!-- 设置按键控制tracking的开始，startVideo()函数是开始整个tracking的循环 --> 
			<div id="text">			 
				<div id="gum" class="gum">
					<p>
					</p>
				</div>
				<div id="nogum" class="nogum">
					<p>
						There was some problem trying to capture your webcamera, please check that your browser supports WebRTC. Using a fallback video instead. To try it out:
						<ol>
							<li>click start</li>
							<li>see the model fitted to the face</li>
						</ol>
					</p>
				</div>
			</div>
			<p id="emotion"></p>  <!-- 这一行paragraph显示emotion信息 -->
			<p id="positions"></p> <!-- 这一行paragraph显示特征点位置信息 -->
			<p id="headpose"></p>  <!-- 这一行paragraph显示头部姿势信息 -->
			<!-- 主要的script -->
			<script>
				var vid = document.getElementById('videoel');//用变量vid代表设置的video信息
				var vid_width = vid.width;
				var vid_height = vid.height;
				var overlay = document.getElementById('overlay');//用变量overlay代表设置的canvas信息
				var overlayCC = overlay.getContext('2d');
				/*********** Setup of video/webcam and checking for webGL support *********/
				function enablestart() {
					var startbutton = document.getElementById('startbutton');
					startbutton.value = "start";
					startbutton.disabled = null;
				}
				var insertAltVideo = function(video) {
					// insert alternate video if getUserMedia not available
					if (supports_video()) {
						if (supports_webm_video()) {
							video.src = "./media/cap12_edit.webm";
						} else if (supports_h264_baseline_video()) {
							video.src = "./media/cap12_edit.mp4";
						} else {
							return false;
						}
						return true;
					} else return false;
				}
				function adjustVideoProportions() {
					// resize overlay and video if proportions of video are not 4:3
					// keep same height, just change width
					var proportion = vid.videoWidth/vid.videoHeight;
					vid_width = Math.round(vid_height * proportion);
					vid.width = vid_width;
					overlay.width = vid_width;
				}
				function gumSuccess( stream ) { //成功调用摄像头之后，给vid传入摄像头获取的数据流
					// add camera stream if getUserMedia succeeded
					if ("srcObject" in vid) {
						vid.srcObject = stream;
					} else {
						vid.src = (window.URL && window.URL.createObjectURL(stream));
					}
					vid.onloadedmetadata = function() {
						adjustVideoProportions();
						vid.play();
					}
					vid.onresize = function() {
						adjustVideoProportions();
						if (trackingStarted) {
							ctrack.stop();
							//htracker.stop();
							ctrack.reset();
							ctrack.start(vid);
							//htracker.start();
						}
					}
				}
				function gumFail() {
					// fall back to video if getUserMedia failed
					insertAltVideo(vid);
					document.getElementById('gum').className = "hide";
					document.getElementById('nogum').className = "nohide";
					alert("There was some problem trying to fetch video from your webcam, using a fallback video instead.");
				}
				navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
				window.URL = window.URL || window.webkitURL || window.msURL || window.mozURL;
				// set up video 这一段调用摄像头以及看是否调用成功
				if (navigator.mediaDevices) {
					navigator.mediaDevices.getUserMedia({video : true}).then(gumSuccess).catch(gumFail);
				} else if (navigator.getUserMedia) {
					navigator.getUserMedia({video : true}, gumSuccess, gumFail);
				} else {
					insertAltVideo(vid);
					document.getElementById('gum').className = "hide";
					document.getElementById('nogum').className = "nohide";
					alert("Your browser does not seem to support getUserMedia, using a fallback video instead.");
				}
				vid.addEventListener('canplay', enablestart, false);
				
				//上面是获取摄像数据的一些设置，通过以上设置，我们获取到的视频数据裁剪处理之后用vid (400 x 300)来表示了，下面对这个数据进行处理。
				
		     	/*********** setup of emotion detection *************/
				// set eigenvector 9 and 11 to not be regularized. This is to better detect motion of the eyebrows
				pModel.shapeModel.nonRegularizedVectors.push(9);
				pModel.shapeModel.nonRegularizedVectors.push(11);
				var ec = new emotionClassifier();
				ec.init(emotionModel);
				       								
				/*** Code for head tracking ***/
				var htracker = new headtrackr.Tracker({calcAngles : true, ui : false});
  				htracker.init(vid, overlay);	
  				htracker.start();	
				document.addEventListener("facetrackingEvent", function(event) {
				//在这里可以对head的数据进行实时处理，x,y是头（脸）的位置(400 x 300)，width,height是头（脸）的像素长宽，angle是头（脸）的角度（正常情况下为90度，也就是pi/2，往左倾斜，变大）
				var headInfo = "";
				headInfo += "Face positon: [" + event.x + ", " + event.y + "]  ";
				headInfo += "width and height: [" + event.width + ", " + event.height + "]  ";
				headInfo += "angle: " + event.angle + "<br/>";
				document.getElementById('headpose').innerHTML = headInfo;
				});
				 			
				/*********** Code for face tracking *********/
				var ctrack = new clm.tracker({searchWindow:11});
				ctrack.init(pModel);
				ctrack.setResponseMode("single", ["raw"]);
				var trackingStarted = false;
				function startVideo() { //按下按钮之后再开始tracking脸和头
					// start video
					vid.play();
					// start tracking
					ctrack.start(vid);
					//htracker.start();
					trackingStarted = true;
					// start loop to draw face and output messages
					drawLoop();
					positionLoop();
				}
				
				/******Code for output positions of face feature points*************************/
				function positionLoop() {
					requestAnimFrame(positionLoop);
					var positions = ctrack.getCurrentPosition();//这个函数返回了检测到的所有特征点(70个)的在vid（400 x 300）中的位置，格式是：positions[i][j]代表第i个特征点的位置(j=0代表x，j=1代表y，注意原点在右上方)
					var positionString = "";
					if (positions) {//列了一些关键点的位置
					/********** Output key position ***********/
						positionString += "left eye: ["+positions[32][0].toFixed(1)+","+positions[32][1].toFixed(1)+"] ";
						positionString += "right eye: ["+positions[27][0].toFixed(1)+","+positions[27][1].toFixed(1)+"]<br/>";
						positionString += "chin point: ["+positions[7][0].toFixed(1)+","+positions[7][1].toFixed(1)+"]<br/>";
						positionString += "nose point: ["+positions[62][0].toFixed(1)+","+positions[62][1].toFixed(1)+"]<br/>";
						positionString += "feature points of eyebrow: 15 - 18 -> left, 19 - 22 -> right" + "<br/>";
						for (var p = 15;p < 23;p++) {
							positionString += "featurepoint "+p+" : ["+positions[p][0].toFixed(1)+","+positions[p][1].toFixed(1)+"]<br/>";
						}
						/********** decide whether the mouth is open ************/
						//这里用嘴的上下左右四个点来判断开合，参考https://github.com/douyamv/FaceTracker/blob/master/main.html 
						var mousedist = positions[57][1] - positions[60][1];
						var mouthwidth = positions[50][0] - positions[44][0];
						var mouthstate = "";
						if(mousedist>mouthwidth*0.2){
							mouthstate = "open";
						}
						else if(mousedist<mouthwidth*0.2){
							mouthstate = "close";
						}
						else {
							mouthstate = "uncertain";
						}
						
						//这里是嘴的四个关键点的位置，关注position[i][j]就好
						positionString += "Mouth: " + mouthstate + "<br/>";
						
						positionString += "feature points of mouth:" + "<br/>";
						positionString += "left: ["+positions[50][0].toFixed(1)+","+positions[50][1].toFixed(1)+"] ";
						positionString += "right: ["+positions[44][0].toFixed(1)+","+positions[44][1].toFixed(1)+"]<br/>";
						positionString += "top: ["+positions[60][0].toFixed(1)+","+positions[60][1].toFixed(1)+"] ";
						positionString += "down: ["+positions[57][0].toFixed(1)+","+positions[57][1].toFixed(1)+"]<br/>";							
					}
					
					var cp = ctrack.getCurrentParameters();//这个函数返回追踪到的parameter，用来预测情绪
					var emotionvalue = "";
					var mostPossible = "";//the max possiblity of all emotions
					var temp0 = 0;//temp parameter for comparing possiblities
					var er = ec.meanPredict(cp); //预测情绪
					//er有两个属性，emotion是情绪名称，value是相应概率，er[0]~er[5]代表angry,disgusted,fear,sad,surprised,happy.
					/***meanPredict, show possibilities of all emotions***/
					for (i = 0; i < er.length;i++){
						emotionvalue += er[i].emotion + ": " + (er[i].value * 100).toFixed(1) + "%<br/>";//e.g. disgusted: 15.1%
						if (temp0 < er[i].value){ //获取最高概率的情绪mostPossible
							temp0 = er[i].value;
							mostPossible = er[i].emotion;	
						}
					}
					//显示相关信息，可忽略
					document.getElementById('positions').innerHTML = positionString;
					document.getElementById('emotion').innerHTML = "Predicted emotions: " + mostPossible + "<br/>" +  "Possibilities of all emotions: <br/>" + emotionvalue;
					}

				function drawLoop() {//画出人脸位置，就是那些绿色的线，用ctrack.draw在overlay这个canvas上画出来
					requestAnimFrame(drawLoop);
					overlayCC.clearRect(0, 0, vid_width, vid_height);
					//psrElement.innerHTML = "score :" + ctrack.getScore().toFixed(4);
					if (ctrack.getCurrentPosition()) {
						ctrack.draw(overlay);
					}
				}
				//暂时忽略。
				/*********** Code for stats **********/
				stats = new Stats();
				stats.domElement.style.position = 'absolute';
				stats.domElement.style.top = '0px';
				document.getElementById('container').appendChild( stats.domElement );
				// update stats on every iteration
				document.addEventListener('clmtrackrIteration', function(event) {
					stats.update();
				}, false);
			</script>
		</div>
	</body>
</html>
